<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>自托管 Obsidian 多端同步服务器</title>
    <url>/2025/05/09/obsidian/</url>
    <content><![CDATA[在使用 Obsidian 进行笔记管理时，多设备同步是一个重要需求。虽然有官方的 Obsidian Sync 服务，但鉴于其价格以及国内的稳定性，自建同步服务器是更好的选择。本文将尝试基于 CouchDB 搭建自己的 Obsidian LiveSync 同步服务器，实现多端笔记实时同步。
什么是 Obsidian LiveSync？Obsidian LiveSync 是一个 Obsidian 插件，允许用户通过 CouchDB 兼容服务器同步多个 Vault 副本。相比官方同步服务，它的优点是：

数据完全存储在自己的服务器。
同步速度快，延迟低。
支持端到端加密。
免费开源。

部署 CouchDB环境准备本文以 Rocky 9 为例，其他 Linux 发行版类似。
安装 CouchDB添加 CouchDB 官方源Rocky Linux 官方仓库没有最新 CouchDB，需要添加 Apache 官方源。  
sudo yum install -y yum-utilssudo yum-config-manager --add-repo https://couchdb.apache.org/repo/couchdb.repo# 启用 EPEL 软件源sudo dnf config-manager --set-enabled crbsudo dnf install epel-release epel-next-release # epel-release 有国内源# 可选：epel-release 换成清华源sudo sed -e &#x27;s!^metalink=!#metalink=!g&#x27; \    -e &#x27;s!^#baseurl=!baseurl=!g&#x27; \    -e &#x27;s!https\?://download\.fedoraproject\.org/pub/epel!https://mirrors.tuna.tsinghua.edu.cn/epel!g&#x27; \    -e &#x27;s!https\?://download\.example/pub/epel!https://mirrors.tuna.tsinghua.edu.cn/epel!g&#x27; \    -i /etc/yum.repos.d/epel&#123;,-testing&#125;.repo# 由于无法同步，镜像站不包含 EPEL Cisco OpenH264 仓库（`epel-cisco-openh264.repo`），如果不需要可手动将其改为 `enabled=0`。

安装 CouchDBsudo yum install -y mozjs78 # RedHat &lt; 9 的不需要安装sudo yum install -y couchdb# 安装可能会比较慢，可以尝试使用代理加速

配置 CouchDB修改默认CouchDB 配置文件路径为 /opt/couchdb/etc/local.ini 
[couchdb]single_node = true                           ; 启用单节点模式max_document_size = 50000000                 ; 允许最大文档大小（50MB）[chttpd]port = 5984                                  ; CouchDB HTTP 监听端口，默认为 5984bind_address = 127.0.0.1                     ; 监听地址，建议设为 127.0.0.1，仅允许本地访问（由 Nginx 反代）require_valid_user = true                    ; 所有请求都必须通过验证（强制登录）max_http_request_size = 4294967296           ; 最大请求体大小（单位字节），此处为 4GB[chttpd_auth]require_valid_user = true                    ; 强制所有请求进行用户验证authentication_redirect = /_utils/session.html ; 浏览器请求未登录时跳转到 Web 登录界面[httpd]WWW-Authenticate = Basic realm=&quot;couchdb&quot;     ; 启用 HTTP Basic 认证提示enable_cors = true                           ; 启用 CORS（跨域资源共享）[admins]admin = mysecretpassword                     ; 管理员用户名及密码（首次初始化使用）[cors]origins = app://obsidian.md,capacitor://localhost,http://localhost                                             ; 允许 Obsidian 移动端/桌面端连接credentials = true                           ; 允许发送认证信息（如 Cookie / Authorization Header）headers = accept, authorization, content-type, origin, referer                                             ; 允许的请求头methods = GET, PUT, POST, HEAD, DELETE       ; 允许的请求方法max_age = 3600                               ; CORS 预检请求缓存时间（单位：秒）[ssl];enable = true;cert_file = /full/path/to/server_cert.pem;key_file = /full/path/to/server_key.pem;password = somepassword

启用 CouchDB 并 开启自启sudo systemctl enable --now couchdb    # 启用开机自启并运行 CouchDBsudo systemctl status couchdb          # 查看运行状态

发现启用 CouchDB 后报错 Error: the _users database does not exist ，说明 CouchDB 正尝试访问其默认的 _users 系统数据库，但这个数据库尚未创建。这个数据库用于用户认证和权限管理，是正常运行所必需的。
解决方法： 手动创建 _users 数据库可以使用命令行工具 curl 或浏览器访问 CouchDB 的 HTTP API 创建这个数据库。
curl -X PUT http://127.0.0.1:5984/_users# 如果 CouchDB 开启了认证，请加上用户名密码（例如 admin/admin）：curl -X PUT http://admin:admin@127.0.0.1:5984/_userssudo systemctl restart couchdb    # 创建完成后，重启 CouchDBsudo systemctl status couchdb     # 查看运行状态
测试访问 CouchDB
curl http://admin:admin@127.0.0.1:5984 # 也可以用浏览器访问
CouchDB 已成功部署并正常运行。
配置 HTTPS 反向代理为了安全访问，建议通过 Nginx + SSL 证书为 CouchDB 提供 HTTPS 支持。
安装 Nginx# 安装 Nginxsudo dnf install -y nginx         # 启动并自启 Nginxsudo systemctl enable --now nginx
配置反向代理编辑 /etc/nginx/conf.d/couchdb.conf ：
# HTTP 重定向到 HTTPSserver &#123;    listen 80;    server_name your.domain.com;    # 可选：防止暴露版本信息    server_tokens off;    return 301 https://$host$request_uri;&#125;# HTTPS 反向代理到 CouchDBserver &#123;    listen 443 ssl http2;    server_name your.domain.com;	# 配置 SSL 证书路径    ssl_certificate     /etc/letsencrypt/live/your.domain.com/fullchain.pem;    ssl_certificate_key /etc/letsencrypt/live/your.domain.com/privkey.pem;    # 安全增强（可选）    ssl_protocols TLSv1.2 TLSv1.3;    ssl_prefer_server_ciphers on;    # 自定义访问日志路径    access_log /var/log/nginx/couchdb.log main;    error_log  /var/log/nginx/couchdb_error.log warn;    location / &#123;        proxy_pass http://127.0.0.1:5984/;        proxy_set_header Host $host;        proxy_set_header X-Real-IP $remote_addr;        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;    &#125;&#125;
配置好之后测试 Nginx ，如果没报错，则重启Nginx
# 检验 Nginx 配置nginx -t## nginx: the configuration file /etc/nginx/nginx.conf syntax is ok## nginx: configuration file /etc/nginx/nginx.conf test is successful# 重启 Nginxsudo systemctl restart nginx
至此，Nginx反向代理配置完成。
Obsidian 配置 LiveSync下载 LiveSync 插件Obsidian - 设置 - 第三方插件 - 社区插件市场 - 浏览
搜索 LiveSync 并安装

配置 LiveSyncObsidian - 设置 - Self-hosted LiveSync - ManialSetup - Start - Remote Configuration检验数据库配置是可能会出现几个告警，可通过点击告警后面的fix来修复，至此第一个设备配置完成。
多端同步设置 - Self-hosted LiveSync - ManialSetup - Setup - To setup other devices 生成同步 URI
在其他设备上安装好 Self-hosted LiveSync 插件后，进入 Setup 页面，点击 Connect with setup URI 并输入同步 URI ，后续选择第一项，确认导入。
同步策略设置 - Self-hosted LiveSync - Sync Settings 可以配置同步策略。同步模式 LiveSync 为实时同步，在跟移动设备同步时，启用 LiveSync 会提升功耗，按需开启。
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>obsidian</tag>
        <tag>工具</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>RAID级别介绍和配置指南</title>
    <url>/2025/05/26/raid/</url>
    <content><![CDATA[RAID 概述RAID（独立磁盘冗余阵列）是一种通过虚拟化技术将多个硬盘组合成一个逻辑存储组，以同时提升存储性能和数据冗余的方法。简单来说，RAID 将多块磁盘联合起来对外呈现为一个更大的磁盘，操作系统将其视为单一的存储设备。根据不同的 RAID 层级，数据会以不同方式分布到各个硬盘上。每种 RAID 级别都有其特定的实现方式和理论上的优缺点，在提高数据可靠性和读写性能两方面各有侧重。
RAID 最初定义了多个标准层级（RAID 0～RAID 5 等），后来又出现了更高层级、组合级别和厂商特定的 RAID 实现。需要注意的是，RAID 并不是备份手段——虽然某些 RAID 级别提供冗余保护，能够在硬盘故障时防止数据丢失，但如果发生数据误删或灾难性损坏，RAID 无法替代离线备份。RAID 的主要目的在于提供磁盘容错能力和提高 I&#x2F;O 性能（如增加吞吐量、提升 IOPS）。
一般情况下，RAID 常用于服务器和存储系统，但随着磁盘价格降低和主板集成 RAID 功能的普及，个人用户在需要大容量或高性能存储时也开始采用 RAID。
RAID 级别分类RAID 级别通常以数字编号区分不同的数据组织方式，包括标准 RAID 级别（如 RAID 0, 1, 2, 3, 4, 5, 6 等）和组合 &#x2F; 嵌套 RAID 级别（如 RAID 10, 0+1, 50, 60 等）。此外，还有厂商或特定文件系统实现的变种（例如 NetApp 的 RAID-DP、ZFS 文件系统的 RAID-Z 等）。
以下将详细介绍常见和非常见的各 RAID 级别，包括其空间利用计算方式、性能特点、容错与恢复能力、优缺点以及典型应用场景，并提供配置方法和 RAID 选择指南。
各 RAID 级别详解RAID 0 – 条带化存储（Striping）RAID 0 通过数据条带化（Striping）实现性能最大化。它将数据分割成块，交替分布在两个或更多磁盘上，使所有磁盘可以并行读写。因此 RAID 0 能够提供线性提升的读写性能：如果有 n 块磁盘，理论上顺序读写速度可以接近单盘的 n 倍。RAID 0 不存储任何校验或冗余信息，没有冗余和容错能力，这意味着其中任意一块磁盘发生故障都会导致整个阵列的数据全部丢失。

空间计算方式：RAID 0 的可用总容量是所有成员磁盘容量的总和。例如，使用 n 块容量相同的硬盘组成 RAID 0，可用容量 &#x3D; n × 单盘容量。没有任何容量损失在冗余上（冗余开销为 0）。
性能特性：RAID 0 在所有 RAID 级别中读写性能最优。读写操作可以由多盘并行处理，顺序吞吐量和并发 I&#x2F;O 能力都成倍提高。对于大文件的顺序读写，吞吐量近似于单盘的 n 倍；对于随机 I&#x2F;O，由于没有校验计算，也无额外开销，性能主要取决于并发 I&#x2F;O 负载能否有效利用多磁盘并行性。
冗余与容错：无冗余。任意一盘故障即阵列崩溃，故障耐受度 &#x3D; 0（不可容忍任何磁盘损坏）。数据恢复几乎不可能，只能通过备份还原，因为 RAID 0 没有校验数据可用于重建。
优点：实现最大的存储利用率和最高的 I&#x2F;O 性能；成本最低（无额外冗余磁盘）。
缺点：数据安全性极低，单盘故障会造成全部数据丢失；MTBF（平均无故障时间）实际上比单盘更低（因为更多盘带来更高故障概率）。
典型应用场景：适用于对性能要求极高且能接受数据丢失风险的场合，例如视频剪辑的高速缓存 &#x2F; 临时处理盘、非关键性的临时计算空间等。这些场景通常有独立的数据备份或源文件，可在阵列故障后重新获取数据。

RAID 1 – 磁盘镜像（Mirroring）RAID 1 通过磁盘镜像实现数据冗余，即将相同的数据实时写入两块或多块硬盘中。每写入一份数据都会同步复制到镜像盘上，从而在所有成员盘保存完整的副本。RAID 1 提供了最高的数据安全性：只要阵列中至少有一块磁盘完好，数据即完整无损。典型情况下 RAID 1 由两块磁盘组成镜像对（也可扩展为多盘镜像，以增加读取性能或额外备份副本）。

空间计算方式：RAID 1 可用容量等于其中最小容量磁盘的容量（相当于仅利用了一块盘的容量）。例如，两块 4TB 镜像，则可用总容量为 4TB；若镜像由大小不同磁盘组成，则以较小磁盘容量为准。由于每份数据都写入两份，磁盘利用率仅 50%（两盘情况下）；用更多磁盘做多重镜像时，利用率更低，是所有 RAID 中磁盘利用率最低的级别。
性能特性：RAID 1 的读取性能良好。因为每块盘上都有完整数据，控制器 &#x2F; 操作系统可以将读取任务分散到多个盘并行完成。在理想情况（多线程读取或智能调度）下，读取吞吐量可接近 n 倍于单盘（n 为镜像盘数）。实际中对于单一顺序读取，部分实现也能通过交替从不同盘读取来略微提升速度。写入性能略低于单一硬盘：需要对每块镜像盘都写入一次，但这些写操作可以并行进行，因此总体写入速度接近单盘水平，开销主要是同步多盘写入的微小延迟。
冗余与容错：RAID 1 具有最高容错能力。只要有一块磁盘正常，数据即不丢失。在两盘镜像中，可容忍 1 盘故障；在 n 盘镜像中，最多可容忍 n-1 块磁盘同时故障（只剩一盘仍可运行）。故障盘更换后，阵列会通过将剩余完好盘的数据拷贝到新盘来重建镜像，恢复满冗余状态。重建过程相对简单（为整盘复制），速度取决于磁盘大小和接口带宽。
优点：提供极高的数据安全性和可靠性，是容错能力最强的 RAID 级别；读取性能有提升（尤其在多用户场景下并发读取效能好）；重建逻辑简单。
缺点：存储利用率低（50% 或更低）；成本高昂（有一半以上的存储投入用于冗余）；写入开销略有增加；无法容忍数据不一致（需确保镜像实时同步，否则可能出现镜像不同步问题，但一般 RAID 控制器会处理同步）。
典型应用场景：适用于数据安全要求最高、小容量且随机写入较多的场景。例如操作系统盘、数据库日志盘、小型企业或个人的重要数据存储、服务器系统盘等。很多个人 NAS 和企业级存储会采用 RAID 1 来保证磁盘任一损坏时数据不丢失。在只有两块磁盘的情况下，RAID 1 往往是唯一可以提供冗余保护的选择。

RAID 2 – 汉明码校验（Hamming Code ECC）RAID 2 是早期的一种实验性 RAID 级别，利用汉明码（Hamming Code）的错误校验和纠正机制来提供数据冗余。它可以看作对 RAID 0 的改良：数据在写入前会编码拆分成位（bit）条带，分散写入多个数据盘；同时计算错误校正码（ECC）存放在额外的校验盘上。汉明码能够检测并纠正单个位错误，因此 RAID 2 可以容忍单一磁盘故障（相当于一整个盘位的数据丢失，可被 ECC 纠正）。若同时有两块或以上磁盘出问题，超出汉明码纠错能力，则阵列数据无法完全恢复。

空间计算方式：RAID 2 需要预留多个磁盘用于存放 ECC 校验位，具体数量取决于数据盘数量和汉明码算法（汉明码需要若干校验位覆盖特定位宽的数据位）。因此阵列总容量略小于数据盘容量之和，而且比 RAID 5&#x2F;6 这类只需 1-2 盘校验的方案开销更大。例如一个简单实现可能用 4 个数据盘 + 3 个校验盘（共 7 盘）来组织汉明码，使得可用容量约为 4 盘总容量，冗余开销较高。总的来说，RAID 2 可用容量会因为存储 ECC 信息而“比原始数据大一些”。
性能特性：RAID 2 进行位级别的条带化，所有磁盘（数据盘和校验盘）在每次读写中都参与操作。一段数据通常分散到位粒度，需要所有盘同步读 &#x2F; 写才能组合 &#x2F; 校验数据。这意味着 RAID 2 无法独立访问单盘完成 I&#x2F;O，即使很小的数据读写也需要整个阵列协同，这使其随机 I&#x2F;O 性能较差、延迟较高。然而在顺序传输时，多盘并行仍带来带宽提升。此外，ECC 计算会带来额外的计算开销，纯软件实现时对 CPU 有较大负载。
冗余与容错：可容忍 1 块磁盘故障（单盘数据可由 ECC 纠正）。当一盘失效时，阵列可通过剩余数据盘和校验盘的汉明码计算重建出丢失的数据位，纠错过程类似于内存 ECC 校验纠正单比特错误的原理。一旦故障盘更换，系统可重新计算该盘的所有 ECC 或数据位进行恢复。如果多于 1 盘同时故障，纠错失败，阵列将崩溃无法正常使用。
优点：引入了先进的纠错码（ECC），相较早期只条带化的方案，有一定的容错能力；在特定实现中，读性能和顺序吞吐仍能受益于多盘并行。
缺点：实现复杂，需专用硬件支持高速的位级条带和 ECC 计算；存储开销大（需要多个校验盘）；随机 I&#x2F;O 性能不佳；实际应用极少。多数现代 RAID 系统没有采用 RAID 2。
典型应用场景：RAID 2 主要具有历史意义，现代实践中几乎找不到实现的 RAID 2 控制器或软件。它更多作为冗余编码思想的展示，用于教科书或研究。现实中更高效的 RAID 5&#x2F;6 等取代了 RAID 2。

RAID 3 – 字节交错奇偶校验（Byte-Interleaved Parity）RAID 3 采用字节（或位）交错的条带化方式，并在一个专用奇偶校验盘上存储校验信息。和 RAID 2 类似，RAID 3 也需要所有成员盘同步工作来完成 I&#x2F;O 操作，但不同的是 RAID 3 使用简单的异或（XOR）校验而非复杂 ECC 码。这意味着数据按字节 &#x2F; 位分布在多个数据盘上，而每个条带的奇偶校验位集中存储在专门的 Parity 盘上。RAID 3 能容忍单盘故障，通过奇偶校验盘的数据与其余数据盘内容进行异或运算即可重建丢失数据。

空间计算方式：RAID 3 至少需要 3 块磁盘（2 个数据盘 + 1 个校验盘）。可用容量 &#x3D; (总磁盘数 - 1) × 单盘容量。也就是说，有一整块盘的容量用于存储奇偶校验，冗余开销约占总容量的 1&#x2F;n（n 为磁盘数）。例如 4 盘 RAID 3 的可用容量为 3 盘数据总和，25% 容量用于校验。
性能特性：RAID 3 的顺序读写性能很好。由于数据被按字节条带到所有磁盘上，读取或写入大块顺序数据时，所有盘可并行传输，不同字节在不同盘上同步读 &#x2F; 写，整体吞吐量接近多盘合计带宽。然而，随机访问性能较差：因为即使读取少量数据，也需要所有数据盘定位并同时读取各自的部分，再组合成完整数据。这导致小 I&#x2F;O 请求无法在盘间并行服务，响应时间受限于阵列中最慢的一块盘。另外，每次写入除了更新数据盘上的对应字节，还需要更新奇偶校验盘，会频繁地对校验盘进行操作。
冗余与容错：可容忍 1 块磁盘故障（数据盘或奇偶盘任意一块）。当任一数据盘损坏时，其上数据可由剩余所有数据盘字节异或结果与校验盘数据计算恢复；当校验盘损坏时，没有数据实际丢失，只需重建新的校验盘内容即可。RAID 3 发生单盘故障后阵列能继续工作，但所有 I&#x2F;O 都会受影响，因为缺盘时每次访问仍需计算或跳过缺失部分的数据，性能下降明显。更换上新盘后，系统通过剩余数据计算重建校验或数据内容，恢复到冗余保护状态。
优点：顺序读写性能高，适合大型流式数据传输；提供基本容错能力（单盘）；实现原理比 RAID 2 简单（仅用 XOR 校验）；磁盘利用率相对可接受（浪费一盘容量）。
缺点：随机 I&#x2F;O 性能低，每次操作都牵涉所有磁盘；存在单一奇偶校验盘瓶颈——校验盘在每次写入时都要更新，成为全阵列最繁忙的盘，可能限制写性能并加速该盘磨损；只能容忍单盘故障，无法应对双盘失效。
典型应用场景：RAID 3 适合以顺序读写为主的场景，如流媒体服务器、视频编辑和播放系统等，在这类场景下需要同时读取大量连续数据。但是由于随机性能不佳和校验盘瓶颈，RAID 3 在通用计算环境中并不常见，已被 RAID 5 等更灵活的方案取代。目前少数专用存储设备或老旧系统可能实现 RAID 3，但新项目中几乎不再选用。

RAID 4 – 独立块奇偶校验（Dedicated Parity with Block Striping）RAID 4 与 RAID 3 类似，也采用一个专门的奇偶校验盘，但数据不是按字节而是按块（Block）进行条带化。这意味着每个磁盘可以独立存储完整的数据块，不同块分散在不同磁盘上，同时在一个固定磁盘上为对应条带存储奇偶校验信息。RAID 4 的关键区别在于块级条带允许每次 I&#x2F;O 定位并作用于单个磁盘上的完整数据块，从而改进了并发性能。

空间计算方式：RAID 4 至少需要 3 盘（2 数据 + 1 校验）。可用容量与 RAID 3 相同，即 &#x3D; (n-1)× 单盘容量。一块盘的容量专用于奇偶校验信息，典型实现中各条带的校验块都在这同一盘上。
性能特性：RAID 4 的读取性能相对于 RAID 3 有显著提升。由于数据按块存储，不同块可能在不同磁盘，因此对不同块的读取可以由不同磁盘独立完成——例如多个用户并发读取不同文件时，I&#x2F;O 请求可以分散到多个盘，各自并行服务，从而提高并发吞吐量。这种独立磁盘读能力是 RAID 3 不具备的（RAID 3 所有 I&#x2F;O 都需全盘参与）。顺序读的性能在 RAID 4 中与 RAID 3 类似（多盘并行）；随机读因能够单盘完成，性能接近单盘随机读乘以并发盘数。写入性能则仍然受到专用校验盘的影响：每次写入一个数据块，需要同时访问校验盘来更新对应校验块。流程通常为“读出旧数据块和对应旧校验块 -&gt; 用新数据计算新的校验值 -&gt; 写入新数据块和新校验块”，由此校验盘的写操作频率极高。结果是奇偶盘成为瓶颈，限制写入吞吐，并导致该盘负载远高于数据盘。
冗余与容错：可容忍单盘故障（与 RAID 3 相同）。数据盘故障可通过余下所有数据盘和校验盘重算恢复，校验盘故障则只需重建校验信息。单盘失效期间，阵列可以运行但性能降低，需要对每次相关读请求进行额外计算。重建新盘过程与 RAID 3 一致，需要遍历整个阵列计算缺失盘的数据或校验。
优点：支持并行读操作，随机读取性能较好；简单易实现（XOR 校验）；容量利用率与 RAID 3 相同但提供了更高读取并发度。
缺点：写入性能受限于校验盘瓶颈，每次写操作都涉及校验盘更新；无法容忍多盘故障；在高 IO 写入场景下，校验盘压力大成为可靠性短板。
典型应用场景：RAID 4 在纯读取密集场景下较 RAID 3 有优势，但其写入瓶颈令实际应用不广泛。历史上 NetApp 等厂商在自研文件系统中曾采用 RAID 4（因为他们有 NVRAM 等手段缓解写入瓶颈），但总体而言，RAID 4 已经被 RAID 5 所取代。现代系统很少直接配置 RAID 4，一般只在讨论 RAID 原理或少数遗留系统中提及。

RAID 5 – 分布式奇偶校验（Distributed Parity）RAID 5 是目前应用最广泛的 RAID 级别之一。它采用分布式奇偶校验技术，将数据条带和奇偶校验条带分布在阵列中所有磁盘上。简单说，在 RAID 5 中不存在固定的“校验盘”瓶颈，校验信息（Parity）和数据一起均匀地分布到每一块磁盘。每一条带（Stripe）内，有一块磁盘存储该条带的奇偶校验块，其余磁盘存储数据块，而且奇偶块的位置在各条带间循环轮换，使每块磁盘上既有数据又有部分条带的校验。这种设计避免了每次写入都集中更新同一磁盘的情况，从而消除了 RAID 4 中校验盘的性能瓶颈。

空间计算方式：RAID 5 至少需要 3 块磁盘。可用容量 &#x3D; (总磁盘数 - 1) × 最小单盘容量。换言之，相当于浪费了一块盘的容量用于存储分布在各盘的奇偶校验信息。例如 4 盘 RAID 5，可用容量约为 3 盘之和（75%利用率）；8 盘 RAID 5 则可用容量约为 7 盘之和（87.5%利用率）。各盘容量不同时按最小盘容量计算总容量。校验开销固定为一盘容量，因此随着磁盘数量增加，空间利用率越高（接近 100% 但永远少一盘容量）。
性能特性：RAID 5 提供了均衡的读写性能和冗余保护，是一种兼顾存储性能与成本的折衷方案。
读取性能：因为数据条带分布在 (n-1) 块盘上（每条带有一盘是校验无实际数据），顺序读取时几乎相当于并行读取 (n-1) 块磁盘的数据，读取吞吐量接近 RAID 0（只比 RAID 0 少用了存储奇偶的那部分带宽）。随机读也能从不同盘并行获取数据，性能通常良好。
写入性能：写入时由于需要更新奇偶校验，有一定开销。对一个数据块的写入，RAID 5 控制器通常需要读取相关条带上旧的数据和旧的校验块，计算出新的校验，然后写入新数据和新校验（称为“读 - 改 - 写”操作）。因此小块随机写入的 IOPS 性能相比单盘会有所下降。不过，对于大块顺序写入（如整条带写满），可以一次性计算整个条带的新校验并写入，效率较高。在有写缓存（如硬件 RAID 卡开启写回缓存）的情况下，RAID 5 的小写入性能也可显著改善。总体来说，RAID 5 的写性能略低于 RAID 0，但通常可达单盘性能的数十 % 以上，读性能接近 RAID 0 水平。


冗余与容错：可容忍 1 块磁盘故障。当阵列中某一盘失效时，其上的数据可由剩余所有数据块和对应的奇偶校验块计算重建。例如，对于任意一条带，如果其中一个数据块丢失，控制器会从该条带的其他数据块和奇偶块异或运算恢复丢失数据。阵列在单盘故障时仍可工作，但会进入“降级模式”：每次读取缺失盘的数据都需要动态计算（影响性能），写入也需更新额外校验。一般会尽快更换故障盘；新盘加入后，RAID 控制器会根据其余盘的数据和校验信息自动重建失去的数据到新盘上。重建期间磁盘负载很高，需要读取所有其余盘的数据，阵列性能会受影响且在此窗口如果再有第二块盘故障将导致数据不可恢复。
优点：存储空间利用率高，冗余成本低（仅损失一盘容量就获得容错能力）；读取性能接近无冗余的条带化方案；能够容忍单盘失效，提供基本的数据安全保障；适用于多种应用，性价比突出。
缺点：只允许单盘故障，若出现双盘同时故障则数据全部丢失；小随机写性能较差（有奇偶校验计算和读改写开销），不适合极高写入 IOPS 需求的场景；重建过程耗时且对存储系统是高压操作，大容量盘在 RAID 5 中重建可能需要数小时到数十小时，在此期间若再发生故障风险很高。
典型应用场景：RAID 5 广泛用于需要平衡性能、容量和可靠性的场合。例如中小企业的文件服务器、数据库服务器（读多写少的负载）、虚拟化存储、备份存储等。许多 NAS 设备支持 RAID 5，也是家庭用户在有 3 块以上硬盘时经常选择的模式。在云服务和企业存储中，RAID 5 常用于对性能要求中等且希望节约成本的存储池。不过随着硬盘容量增大、重建时间增长，在要求更高可靠性的场景下，RAID 6 正逐渐取代 RAID 5 的位置。

RAID 6 – 双重奇偶校验（Double Distributed Parity）RAID 6 进一步增强了 RAID 5，通过存储两套独立的奇偶校验信息来提供更高的数据保护能力。简单来说，RAID 6 在每个条带中有两个不同的校验块（例如常用 P+Q 双校验，其中 P 可以理解为类似 RAID 5 的异或校验，Q 为基于高级算法如里德 - 所罗门编码的第二校验）。这使得 RAID 6 即使同时有两块磁盘损坏，仍可利用剩余数据和两套校验信息完整恢复数据。

空间计算方式：RAID 6 至少需要 4 块磁盘。可用容量 &#x3D; (总磁盘数 - 2) × 最小单盘容量。也就是相当于有两块盘的容量用于存储校验信息（分布在所有盘上）。例如，6 盘 RAID 6 的可用容量为 4 盘之和（损失 2 盘容量用于冗余，利用率 ≈ 66.7%）；10 盘 RAID 6 则可用 8 盘容量（利用率 80%）。相较 RAID 5，RAID 6 增加了一份校验冗余，因此在相同磁盘数量下总可用空间略少。
性能特性：RAID 6 的读取性能与 RAID 5 类似，读取时主要访问数据盘，两个校验盘位通常不参与正常读取（除非做数据校验或发生故障时重构），因此顺序读吞吐量约等于 (n-2) 块盘并行，接近 RAID 0 性能；随机读也能并行处理多个请求，性能良好。写入性能相对 RAID 5 进一步下降一些，因为每次写入需要计算和更新两种校验。写入一个数据块可能涉及读取旧数据及两份旧校验、计算两份新校验，再写入新数据和新校验共三个盘的写操作（被称为“读 - 改 - 写”开销）。因此 RAID 6 的小块写入有更高的 I&#x2F;O 和计算开销（写入代价比 RAID 5 更大）。在没有硬件加速的情况下，RAID 6 的写性能对 CPU 和总线负荷要求更高，通常更适合由硬件 RAID 卡实现。如果有高效的 RAID 卡或启用写缓存，RAID 6 的写性能可被优化到接近 RAID 5 水平。
冗余与容错：可容忍任意两块磁盘同时故障。这是 RAID 6 最大的优势。当一块盘损坏时，RAID 6 与 RAID 5 类似，可通过剩余数据 + 两校验之中的一份来重建数据；当在重建尚未完成期间又有第二块盘损坏，剩余的数据加两份校验仍然足以恢复这两块损坏盘的数据。只有当同时超过两块磁盘损坏（例如第三块盘在前两块未恢复前损坏）时，阵列才会崩溃丢失数据。RAID 6 大大降低了在大容量阵列中重建过程中再次发生盘损坏而导致数据彻底丢失的风险。更换坏盘后，系统会利用剩余盘上的数据和双校验重新计算并写入新盘，重建过程较 RAID 5 更耗时一些（因为计算更复杂、写入更多）。但总体重建流程与 RAID 5 类似，只是需要处理两套校验。
优点：提供比 RAID 5 更高的安全性，可耐受双盘失效，适合对数据可靠性要求高的应用；读取性能依然接近 RAID 0；容量利用率在阵列较大时依然可观（例如 10 盘阵列有 80% 容量可用，比镜像方案效率高得多）。
缺点：实现和计算复杂度更高，小写入性能较弱（双校验更新带来更大写入惩罚）；需要至少 4 盘起步，灵活性略差；重建时间更长。同时，RAID 6 对硬件要求较高，一般需要专用 RAID 卡或强大的 CPU 来避免性能瓶颈。
典型应用场景：RAID 6 常用于中大型存储阵列中，特别适合需要大容量且强调高可靠性的场景，如企业级存储系统、文件服务器、备份和归档系统、大型 NAS&#x2F;SAN 阵列等。当单阵列中磁盘数量较多或使用了大容量磁盘时，RAID 6 是比 RAID 5 更安全的选择，因为双盘失效的概率在长时间重建中不可忽视。许多硬件 RAID 控制器广泛支持 RAID 6，现代存储方案（如分布式存储里的数据保护）也借鉴了双冗余校验的思想。

RAID 0+1 （RAID 01）– 先条带后镜像的组合RAID 0+1（常称 RAID 01）是 RAID 0 和 RAID 1 的嵌套组合，实现上先进行条带化（RAID 0），再对条带化结果做镜像（RAID 1）。假设有 6 块盘构成 RAID 01，首先将数据在 3 块盘上条带存储（RAID 0 获得高性能和容量），然后使用另外 3 块盘镜像这 3 块数据盘（提供冗余）。换言之，RAID 01 把所有磁盘分成两组，各自先做 RAID 0，然后两组之间再做 RAID 1 作为镜像副本。

空间计算方式：RAID 01 的可用容量等于阵列中一半磁盘容量之和（因为另一半用于镜像）。比如 6 盘 RAID 01，由于一半做条带数据盘，一半做镜像盘，可用容量为 3 盘容量总和（利用率 50%）。总体容量利用率与 RAID 10 类似，均是 50%（当使用对称分组时）。
性能特性：RAID 01 正常情况下的性能与 RAID 0+RAID 1 类似：读取可以利用镜像的并行，理论上有一定提升；写入需要将条带化的数据写到两个镜像组中，写性能略低于同等 RAID 0，但仍保持较高水平。顺序读写性能基本接近 RAID 0，随机读在多线程场景下可有所提升，随机写由于镜像需要双写入，性能与 RAID 10 相近。总的来说，RAID 01 在无故障时性能与 RAID 10 相差不大，属于高性能阵列。
冗余与容错：可容忍至少一块磁盘故障，但容错机制较微妙：当 RAID 01 中一块磁盘损坏时，该盘所在的 RAID 0 组整体失效，因为 RAID 0 无冗余。此时整个阵列退化为另一组 RAID 0（即只有镜像的另一半还在）。举例来说，6 盘 RAID 01（两组 RAID 0，每组 3 盘）中，任意一盘坏，会导致该盘所在的整个 RAID 0 组的数据不可用，相当于一下损失了该组的 3 盘数据，但由于有镜像，另一 3 盘组还有一份完整的数据，可继续提供服务。然而在这种情况下，阵列变为无冗余状态（只剩下一个 RAID 0 在运行）。如果再损坏任何一块盘（不论在哪组，因为一组已经全失效，另一组任何盘坏都直接令阵列崩溃），则阵列数据彻底丢失。因此 RAID 01 在第一次故障后变得非常脆弱，只能容忍任意一组中的单盘故障。如果恰巧第一组有盘坏了，第二组绝不能再坏。总故障容忍度等价于只能承受一次单组故障。
优点：在无故障时提供了与 RAID 10 接近的高性能和高吞吐量；实现和理解相对简单（就是 0 和 1 两级嵌套）。
缺点：容错能力较差，相比 RAID 10 更容易在多盘故障时失效。一次故障后整个阵列立即降为 RAID 0 状态，失去冗余保护；磁盘利用率低（50%）；需要较多磁盘数（至少 4 个）才能实现。
典型应用场景：由于 RAID 01 在故障后的安全性不佳，实际应用中很少直接采用 RAID 0+1。一般只有在某些对性能要求极高且已另有备份的数据场景下才可能使用（例如将两个已有 RAID 0 卷做镜像以提高可用性），但更多情况下人们会选择 RAID 10 作为替代，因为 RAID 10 提供更好的容错。总的来说，RAID 01 作为理论存在多于实践应用。

RAID 1+0 （RAID 10） – 先镜像后条带的组合RAID 10（又称 RAID 1+0）是最常用的嵌套 RAID 之一，它通过先做镜像、再做条带的方式，将 RAID 1 和 RAID 0 的优势结合在一起。实现上，RAID 10 先将所有磁盘两两组成镜像对（RAID 1 提供冗余），然后再将这些镜像对作为独立“盘”条带化（RAID 0 提供性能和容量）。例如，有 4 块盘时，RAID 10 会先组成两个镜像（每对 2 盘），再将两个镜像组合成条带；8 块盘则组成 4 对镜像，再 4 组做条带，以此类推。RAID 10 常被誉为“两全其美”的方案，具有 RAID 0 的高速性能和 RAID 1 的镜像安全性。

空间计算方式：RAID 10 的可用容量约为阵列中一半磁盘容量总和。同 RAID 01 一样，每两块盘做镜像，只能使用其中一盘容量；镜像对数量为磁盘数的一半，然后所有镜像对再条带，总容量 &#x3D; 镜像对数 × 单对容量。简而言之，利用率 50%（偶数盘情况下）。例如 8 盘 RAID 10，可用容量等于 4 盘容量之和（其余 4 盘用于镜像冗余）。
性能特性：RAID 10 在性能方面表现优异且稳定。因为数据在每对镜像中有两份，读取时控制器可以智能地从中选择更快的一份读取，或者并行读不同镜像以提高总吞吐。顺序读取 &#x2F; 写入性能接近 RAID 0 条带化的水平（使用一半数量的盘，因为另一半是镜像冗余）。例如 8 盘 RAID 10 的顺序吞吐量大致相当于 4 盘 RAID 0，因为 8 盘组成 4 个镜像对再条带。随机读取性能尤其突出：多个镜像可以同时处理不同的读请求，随机读 IOPS 在多线程情况下远超单盘。写入性能则与镜像类似：每对镜像内部需要写入两份，但这些镜像对之间是并行条带的关系，所以整体写入吞吐仍可接近多盘并行。对比 RAID 5&#x2F;6，RAID 10 的写入没有奇偶校验开销，随机写性能明显更好，延迟更低，非常适合高 I&#x2F;O 应用。
冗余与容错：RAID 10 具有较高的容错能力和可靠性。它能容忍的故障数取决于故障分布：只要不是同一镜像对的两盘都坏，阵列就能保持数据完整。具体来说，每个镜像对可以容忍 1 盘故障。如果不同镜像对各损坏一盘，RAID 10 仍能正常运行；理论上对于有 m 对镜像的 RAID 10，最理想情况下可容忍 m 块盘分别在不同对中损坏（每对坏 1 盘）而不丢失数据。不过，如果某一镜像对两盘全部故障，那组数据无副本可用，整个阵列即失效。因此最坏情况下 RAID 10 只能容忍与 RAID 1 相同的 1 对盘故障（镜像对内双盘坏则崩溃）。但相比 RAID 01，RAID 10 在第一次故障后的安全性更高：假设 8 盘 RAID 10 有 1 盘故障，则仅影响该镜像对，其他镜像对不受牵连，阵列仍以 RAID 1 模式保护剩余数据。除非同一对的第二盘在重建完成前故障，否则数据仍然安全。这种故障模式显著优于 RAID 0+1。在替换掉故障盘后，控制器会将其镜像对的另一盘数据拷贝到新盘进行重建，恢复成完整镜像。重建期间只需要读取该对中完好盘即可，速度较快且对其它盘无额外压力。
优点：兼具高性能和高可靠性：读写速度接近 RAID 0，故障耐受接近 RAID 1；无奇偶校验计算，延迟低；在多盘阵列中仍有弹性的容错能力（非同组故障不丢数据）；重建快速。
缺点：存储利用率低（50%）；磁盘数量要求至少 4 个，扩展成本高；相比单纯 RAID 5&#x2F;6，容量开销较大（适合磁盘容量充裕情况下使用）。
典型应用场景：RAID 10 非常适合需要高速随机读写且要求高可用性的场景。例如 OLTP 数据库、大型事务系统、虚拟化环境（VM 存储）、高负载邮件服务器等。这些场景下 RAID 5&#x2F;6 的写入性能可能无法满足要求，而 RAID 10 提供了出色的 I&#x2F;O 性能和冗余保障。很多企业级数据库和 I&#x2F;O 密集应用推荐使用 RAID 10。由于其优秀的性能和冗余，RAID 10 也被用于一些中高端 NAS&#x2F;SAN 中以支撑关键业务。

RAID 50 – 条带化的 RAID 5 组合RAID 50（有时称为 RAID 5+0）是将多个 RAID 5 阵列再做条带化的组合级别。实现方式通常是先将磁盘分成若干组，每组独立组成 RAID 5 阵列，再将这些 RAID 5 组按照 RAID 0 方式跨区条带。例如，最小的 RAID 50 可以用 6 块盘构成：先分成两组各 3 盘做 RAID 5（每组有 1 盘作为奇偶校验），得到两个 RAID 5 逻辑盘；然后再对这两个逻辑盘做条带（RAID 0）。这样的结构同时利用了 RAID 5 的冗余和 RAID 0 的性能拓展。

空间计算方式：RAID 50 的可用容量取决于子阵列个数和每组大小。一般公式为：可用容量 &#x3D; 总磁盘数 - 每组 RAID5 的组数（因为每组浪费一块做校验）。如果总盘数为 N，分成 g 组，则每组大小 k &#x3D; N&#x2F;g，每组有效盘数 (k-1)，总有效盘数 &#x3D; g(k-1) &#x3D; N - g。例如上述 6 盘例子：分 2 组 RAID5，每组 3 盘（有效 2 盘），两组总有效盘 4，容量利用率 &#x3D; 4&#x2F;6 ≈ 66.7%。再如 9 块盘如果做 RAID 50，可以分 3 组每组 3 盘 RAID5，则可用容量 &#x3D; 9 - 3 &#x3D; 6 盘容量（利用率 ≈ 66.7%）；若分成 2 组一组 5 盘一组 4 盘则不太均衡，一般会尽量平均分组。与单组 RAID 5 相比，RAID 50 由于每组都有奇偶开销，总的冗余开销略高。例如 9 盘单 RAID5 利用率约 88.9%，而 3 组 3 盘 RAID50 利用率只有 66.7%。但更常见的是在有很多盘时考虑 RAID 50，以避免单一 RAID 5 组过大导致重建风险增高。
性能特性：RAID 50 将 RAID 5 阵列进行条带化，提高了并行 I&#x2F;O 性能。
读取性能：多个 RAID 5 组并行提供数据，顺序读写可以从多个组同时进行，理论吞吐比单 RAID 5 高。同时各组内部又有条带，随机读可同时访问不同组，提高并发能力。
写入性能：RAID 50 的小写入性能仍然受限于 RAID 5 的“读改写”机制，每组内部写入需要计算奇偶校验，但因为存在多个组，并发写入可以分散到不同组，缓解了单一 RAID5 在高并发写入时的瓶颈。总体来说，RAID 50 的顺序吞吐量和 IOPS 均高于单一 RAID 5 阵列，接近 RAID 0 等级；但在单一写入线程情况下，其写性能仍需执行 RAID5 校验计算，不如 RAID 0&#x2F;10。


冗余与容错：RAID 50 可以看作提供一定多盘容错能力：每个子阵列 RAID 5 可容忍 1 盘故障，因此只要每个子阵列故障盘不超过 1 个，RAID 50 整体数据不丢失。例如 2 组 RAID5，即使每组各坏 1 盘（总坏 2 盘），阵列仍然可用。但如果任意一组中有 2 盘失效（超出 RAID5 冗余能力），则该组数据不可恢复，继而整个 RAID 50 失效。因此 RAID 50 的实际容错取决于故障分布：不同组可以各坏一盘而无事，但如果多盘故障落在同一组就危险。相较单一大 RAID 5 组只能容忍 1 盘失效，RAID 50 在理想情况下容忍度提升（比如 4 组 RAID5 理论可容忍 4 盘各在不同组故障）；但是容忍的前提是故障分散。重建过程也是分组进行：某盘故障仅需要其所在 RAID5 组进行校验重建，其余组不受影响。这意味着重建压力可以隔离在较小范围内，降低对整个阵列性能的冲击。
优点：相比单一大型 RAID 5，RAID 50 在性能和可靠性上都有提升：并行条带提高吞吐，并发重建降低风险；可一定程度容忍多盘故障（各组各 1 盘）；重建时间缩短（每组独立重建更快）。
缺点：有效容量比同等磁盘数做 RAID 5 更低（因为多个组多次损失校验盘容量）；实现结构更复杂，配置不当可能导致容量浪费（需要均衡分组）；仍无法容忍单组内的双盘失效；相比 RAID 10，RAID 50 在双盘不同组故障下虽然还能运行，但数据安全性不如双镜像可靠。
典型应用场景：RAID 50 适用于具有大量磁盘的存储系统，希望在 RAID 5 的空间效率基础上增强性能和可靠性。例如一些大容量的盘柜 &#x2F; 磁盘阵列，可能有 20 块盘以上，为降低单组 RAID 5 重建风险和提升性能，可以考虑将其分成 2 或更多组 RAID5 再条带。常见用于数据仓库、大型 NAS 设备、视频监控存储等需要高顺序吞吐且容量较大的场景，也有用于某些服务器的本地存储。当磁盘数量较多时，与其做一个 RAID 5，不如拆分为多个 RAID5 做 RAID50 会是更好的折中。

RAID 60 – 条带化的 RAID 6 组合RAID 60（RAID 6+0）与 RAID 50 类似，是将多个 RAID 6 阵列条带化后的组合级别。实现上，先将磁盘分成若干组，各组做 RAID 6（具有双奇偶校验容错），再将这些组作为独立单元进行 RAID 0 条带。例如，8 块盘可做成两个 4 盘 RAID 6 组，然后两组条带化得到 RAID 60；典型配置常用更多盘，例如 16 盘可分成两组 8 盘 RAID6 或 4 组 4 盘 RAID6，再条带化。

空间计算方式：RAID 60 至少需要8 块盘（因为每组 RAID6 最少 4 盘，两组起条带）。可用容量 &#x3D; 总盘数 - 2×(组数)。即每组有 2 盘容量用于校验，总的校验盘开销是 2×组数。例如，8 盘分两组 4 盘 RAID6，可用容量 &#x3D; 8 - 2×2 &#x3D; 4 盘容量（利用率 50%）；16 盘分两组 8 盘，则可用容量 &#x3D; 16 - 2×2 &#x3D; 12 盘容量（利用率 75%）；如果 16 盘分 4 组 4 盘，则可用容量 &#x3D; 16 - 2×4 &#x3D; 8 盘容量（利用率 50%）。由此可见 RAID 60 的容量利用率取决于每组大小，组越大利用率越高，但过大组会牺牲可靠性优势。
性能特性：RAID 60 具有优异的顺序吞吐性能。多组 RAID 6 并行条带，顺序读写可以同时从多个组获取数据，带宽比单一 RAID 6 更高。每组内部有双校验，单组顺序性能略逊于 RAID 5，但多个组并行可以弥补甚至超越单组性能。随机性能方面，读取并发性和 RAID 50 类似，多组可同时服务多个请求；写入性能受 RAID 6 双校验影响，每组内部小写的开销较大，但并行组仍有助于提高总 IOPS。在重负载下，RAID 60 比单一 RAID 6 能提供更好的性能扩展，因为 I&#x2F;O 压力分摊到多个子阵列上。
冗余与容错：RAID 60 可以容忍每个子阵列中最多 2 块磁盘故障。因此只要每组损坏盘数 ≤2，整个阵列数据保持完好。例如 2 组 RAID6 的 RAID 60，即使总共坏到 4 块盘（每组各 2 块），仍然不丢失数据。但如果任一组中出现第 3 块磁盘故障，该组将崩溃，从而导致整个阵列失效。这种情况发生的概率极低，但要注意 RAID 60 虽然提高了可靠性，仍非无限安全。同样地，多盘故障的影响取决于分布：RAID 60 能承受多块盘同时失效，只要不超过两盘落在同一组内。相比单一大型 RAID 6 阵列只能抗 2 盘坏，RAID 60 在一定程度上降低了多盘连续故障导致数据丢失的风险。重建时，每个故障盘在其所属的小组内进行双奇偶校验恢复，其余组不受牵连，这比在一个超大 RAID6 中重建要更快也更安全（重建并行且数据划分在更小组里）。
优点：非常高的容错能力，可耐受多盘故障（各组可各坏 2 盘）；读取性能极佳，多组并行读写提升吞吐；相比超大 RAID6，重建压力分散，重建速度加快，总体可靠性提升。
缺点：可用容量比单一 RAID6 稍低（多个组有多份校验开销）；实现复杂，配置需要合理规划组大小和平衡；小写入性能依然受限于双重校验开销；需要较多磁盘起步（通常只有在磁盘数量很多时才考虑 RAID 60）。
典型应用场景：RAID 60 多见于超大容量存储服务器或盘阵中，比如动辄几十块盘的大型 SAN&#x2F;NAS。当单个 RAID6 包含过多磁盘时，重建时间漫长且双盘故障风险增加，采用 RAID 60 分组能缓解这些问题。典型应用如不方便频繁备份的大型视频监控存储、海量数据归档、高性能计算集群存储等，这些场景需要既防止多盘故障又兼顾性能。总之，在拥有非常多硬盘并要求极高数据冗余的情况下，RAID 60 是比 RAID 50 更安全的方案，但其复杂性和容量代价也更高。

JBOD – 独立磁盘 &#x2F; 并列盘组 (Just a Bunch of Disks)JBOD 并非严格的 RAID 级别，而是一个术语，意思是“一堆磁盘”（Just a Bunch of Disks）。JBOD 通常指两种情况之一：其一是多个磁盘独立存在，不做任何 RAID 或逻辑组合，每块磁盘各自作为单独卷使用；其二是将多个物理磁盘的空间简单相加合并成一个大逻辑卷（有时称为 Span 或 Concatenation 模式）。后者在某些操作系统 &#x2F; 控制器中也被称为 JBOD 模式，即把多块磁盘顺序拼接成一个虚拟盘，以增加单一卷的容量。

空间计算方式：如果是独立磁盘模式，每块盘容量不变，总容量为各盘容量总和，但分别呈现为多个卷；如果是拼接模式，所有盘容量简单累加呈现为一个卷。因此 JBOD 模式没有冗余开销，可用空间等于磁盘容量总和（拼接时需要所有盘尺寸相加）。需要注意拼接卷的单一文件系统跨越多盘，一旦其中一盘故障，该卷的数据完整性受到破坏。
性能特性：JBOD（独立盘）不提供任何性能聚合，每块磁盘性能独立；在拼接模式下，数据顺序写满第 1 盘再写第 2 盘，以此类推。因此对于顺序 I&#x2F;O，当写满前一盘转移到下一盘时，可能出现短暂延迟，总体性能基本与单盘相当。没有并行条带，也无校验计算，IOPS 和吞吐不会超出单盘范围（除非多个独立盘并行服务不同请求，那只是因为操作系统看到多卷并调度它们并行）。换句话说，JBOD 不提升性能，也不会像 RAID 那样有写入惩罚或校验开销。它的性能特征近似于 RAID 0 在没有并行优势的情况下：没有冗余也没有性能优化。
冗余与容错：JBOD 不提供冗余。独立磁盘场景下，每块盘故障只影响该盘的数据，其它盘不受影响（因为没有关联）；拼接卷场景下，任意一盘故障都会导致整个逻辑卷的数据不完整（卷将损坏，位于故障盘之后的数据全部丢失）。因此 JBOD（拼接模式）的故障域和 RAID 0 相当：一盘坏则整个卷不可用。数据恢复只能针对单盘进行（独立模式）或尝试部分恢复（拼接模式通常难以恢复完整卷，只能根据备份还原）。
优点：实现简单，无需专门 RAID 控制；容量利用率 100% 没有损失；可以混合不同大小的磁盘拼接利用每一字节空间；对于一些不需要性能提升且更关注容量利用的场合，是种灵活选择。独立盘方式下某盘故障不影响其他盘的数据。
缺点：无任何冗余保护，可靠性低；拼接后的卷一盘损坏可能导致整个卷文件系统崩溃；不提供性能提升；管理上多个独立盘缺乏整合，或者拼接盘对故障非常敏感。
典型应用场景：JBOD 常用于临时性或非关键数据存储，以及需要将若干杂余硬盘容量汇聚起来的情况。例如个人用户将数块旧硬盘拼成一个大卷来存放媒体文件、下载文件等非重要数据；或者一些简单备份，把不同数据分别存储在不同独立磁盘上。许多 NAS 设备提供 JBOD 模式供用户在不需要 RAID 时使用全部容量。但总的来说，JBOD 由于缺乏冗余，在任何重要场景下都应谨慎对待，确保有额外的数据备份。

RAID-DP – 双奇偶校验的专有实现 (NetApp RAID-DP)RAID-DP（Double Parity RAID）是存储厂商 NetApp 引入的一种专有 RAID 实现，可视为 RAID 6 的一种变体。RAID-DP 使用了双重奇偶校验机制来提供更高的数据保护能力，能够在两块磁盘同时失效的情况下仍然保证数据完好。与标准 RAID 6 类似，RAID-DP 在每个 RAID 组中指定了两块盘作为奇偶校验盘，用来存储两种不同算法计算的校验信息。不同之处在于，NetApp 的 RAID-DP 基于其 WAFL 文件系统和 RAID4 架构做了优化，使性能损失较小。

空间计算方式：RAID-DP 每个阵列组拥有两块专用奇偶校验盘。可用容量 &#x3D; (总盘数 - 2) × 单盘容量，和 RAID 6 相同的计算方法。如果一个 NetApp 聚合（aggregate）使用 RAID-DP，例如有 10 块盘，则其中 2 盘为奇偶校验，8 盘用于数据（利用率 80%）。NetApp 通常建议 RAID-DP 组大小在一定范围内（如 12～20 个 HDD 为一组），以平衡奇偶开销和重建时间。
性能特性：NetApp 宣称 RAID-DP 的性能与 RAID 10 相近，在多数情况下并不会比 RAID 4&#x2F;RAID 5 有明显下降。这是因为 NetApp 的实现利用 NVRAM 等机制，优化了写入路径，避免了传统 RAID 6 那种每次更新需要大量同步 IO 的问题。在实际应用中，RAID-DP 的顺序读写性能与 RAID 4 接近（因为本质架构仍是类似 RAID4，只是多一个校验盘）；随机写由于双校验，需要略多计算，但 NetApp 通过并行和缓存将其影响降到极小。总体而言，在 NetApp 存储系统上，启用 RAID-DP 不会对性能造成显著瓶颈，同时显著提高了容错能力。
冗余与容错：RAID-DP 能容忍同时两盘失效而数据不丢失。这一点与 RAID 6 一致。在 NetApp 系统中，RAID-DP 已成为默认 RAID 类型。当一盘损坏时，利用剩余数据 + 2 校验可恢复；两盘损坏亦可通过剩余盘和校验重建两盘数据。只有当超过两盘同时损坏或在重建过程中又发生第三盘故障，数据才会不可恢复。NetApp 的 RAID-DP 还配合了热备盘（Hot Spare）和预防性故障检测等策略，进一步降低了阵列崩溃概率。
优点：提供接近 RAID 10 的性能以及远高于 RAID 5 的可靠性（双盘冗余），在企业存储环境中非常实用；相比镜像冗余，存储效率高得多（仅 2 盘冗余开销可保护大量数据盘）；NetApp 实现有许多优化使得重建快速且对业务影响小。对于需要高可靠性又要兼顾容量利用的场景，RAID-DP 是一个理想选择。
缺点：作为 NetApp 的专有技术，RAID-DP 并非所有平台都支持；其理念类似 RAID 6，因此如果脱离 NetApp 环境，软件实现 RAID-DP 的性能会有与一般 RAID 6 相似的开销；只能由特定存储系统使用，灵活性不如通用 RAID 方案。
典型应用场景：RAID-DP 几乎应用于所有 NetApp ONTAP 存储系统中，从中端的 FAS 阵列到高端存储，都默认采用 RAID-DP 来构建底层 RAID 群组，以保护企业数据安全。适合需要极高可用性且使用 NetApp 方案的场景，比如大型企业文件存储、虚拟化环境、数据库存储等。在 NetApp 之外，一般用“RAID6”实现类似功能；但 NetApp RAID-DP 的成功也影响了业界对双奇偶校验 RAID 的接受度，如今双校验已成为企业级存储的标准配置之一。

RAID-Z – 动态条带的 ZFS RAID 实现RAID-Z 是开源 ZFS 文件系统中特有的冗余磁盘阵列实现，可以看作对应于传统 RAID 5&#x2F;6 的新型变种。RAID-Z 的设计目标是在提供奇偶校验冗余的同时消除 RAID 5 的“写入漏洞”问题。ZFS 结合文件系统和设备管理，将写入操作整合成事务，每次写入时动态决定条带宽度，使所有写入都是全条带写入。因此不会出现传统 RAID 在未写满一个条带时更新奇偶校验可能导致的不一致问题（即 RAID-5 Write Hole）。
RAID-Z 根据冗余程度不同分为三级：RAID-Z1（单奇偶校验，相当于 RAID5）、RAID-Z2（双奇偶校验，相当于 RAID6）、RAID-Z3（三奇偶校验）。

空间计算方式：RAID-Z1&#x2F;Z2&#x2F;Z3 的可用容量类似于 RAID5&#x2F;6 逻辑：分别损失 1、2、3 块盘空间用于冗余校验。例如，6 盘 RAID-Z2 可用容量约等于 4 盘总和（损失 2 盘容量）；10 盘 RAID-Z3 可用容量约等于 7 盘总和（损失 3 盘容量）。ZFS 要求一个 RAID-Z vdev（类似于 RAID 组）的所有成员盘大小最好一致，否则会浪费额外空间填充对齐。在容量计算上，ZFS 还允许动态扩展（RAID-Z Expansion 功能，未来可以通过增加盘来扩容阵列）。
性能特性：RAID-Z 的读取性能与传统 RAID 5&#x2F;6 类似，在单块盘发生故障前，读取通常只访问数据盘，不需额外计算；顺序读取多个盘并行，性能接近条带化阵列。写入性能方面，由于 RAID-Z 确保每次写入都是完整条带写入（通过收集足够数据或填充来避免部分条带更新），因此避免了小写入反复读改写校验的开销。这使 RAID-Z 在应对随机同步写入时比传统 RAID 5 有更好的性能和一致性。同时 ZFS 的 ARC 缓存和写日志（ZIL）也提高了写入效率。总体来说，RAID-Z 的性能相对于 RAID 5&#x2F;6 有所改进，尤其在崩溃后无需担心数据和奇偶不一致，可直接继续工作或回滚事务，可靠性和性能稳定性更佳。
冗余与容错：RAID-Z1 可容忍 1 盘故障，RAID-Z2 可容忍 2 盘故障，RAID-Z3 则可容忍 3 盘同时故障。这对应于各自奇偶校验的数量。发生故障盘时，ZFS 会利用奇偶数据和其他盘数据在线重建丢失的数据块到热备盘或新盘。在重建过程中，RAID-Z 依靠 ZFS 的校验和机制，可以确保读取出的数据正确性（ZFS 每个块都有校验和，能发现数据损坏）。如果再有盘损坏超过冗余能力，则该 vdev 崩溃，涉及那个 vdev 的数据池部分不可用。不过 ZFS 通常可以由多个 vdev 组成池，单个 vdev 挂掉相当于池的一部分数据丢失，整个池也就失效了。
优点：消除了 RAID-5 经典问题（写入漏洞），数据一致性更有保障；深度集成文件系统的优势，支持数据自校验、自动修复；提供从 1 到 3 盘的冗余选择，灵活性比传统 RAID 高；写入性能针对小 IO 有所优化；还能在线扩展阵列容量（新版本 ZFS 支持 RAID-Z 逐盘增加扩容）。
缺点：RAID-Z 对内存要求较高（ZFS 整体设计需要大量内存做缓存）；只支持在 ZFS 文件系统环境中使用，不像硬件 RAID 那样通用；当前 RAID-Z 扩容限制较多（需要一次性增加整组盘才能扩容，直到最近的新特性才允许逐个增加盘位）；ZFS 管理和调优有一定学习成本。
典型应用场景：RAID-Z 广泛用于开源 NAS 和数据存储领域，例如 FreeNAS&#x2F;TrueNAS 等 ZFS 系统中。个人和企业用户利用 ZFS 的先进特性（快照、压缩、数据完整性）来构建可靠存储，RAID-Z 为其提供了类似 RAID 的冗余保护。常见场景包括家用 NAS、大容量备份服务器、虚拟机数据存储、容器平台持久化存储等，特别是对数据完整性要求高的地方。RAID-Z2 是很多 NAS 配置中的推荐方案，因为它在容量和安全性上平衡较好；RAID-Z3 则用于需要极高安全性的环境（比如大型存储阵列防止三盘故障）。总的来说，如果采用 ZFS，RAID-Z 系列是首选的冗余方案。

各 RAID 级别特性对比下面的表格总结了主要 RAID 级别的关键特性，包括所需最少磁盘数、最大容错（可容忍的同时故障磁盘数）、可用容量计算、以及相对的读写性能表现和典型应用场景，便于对比不同方案：



RAID 级别
最少硬盘数
最大容错 (故障盘)
可用容量 (以单盘容量 S 计)
读性能 (顺序 &#x2F; 并发)
写性能 (顺序 &#x2F; 随机)
典型应用场景及特点



JBOD
1
0（无冗余）
n × S（所有盘之和）
≈ 单盘 (无并行提升)
≈ 单盘 (无校验开销)
非关键数据的容量汇聚，个人临时存储。无性能增益，故障风险同 RAID0。


RAID 0
2
0（无冗余）
n × S
n × 单盘（顺序）
n × 单盘（顺序）
并发读 &#x3D; n，随机写 ≈ 单盘。追求最大性能和容量的场景，如视频编辑临时盘、高速缓存等。不怕数据丢失。


RAID 1
2
n-1（镜像盘数 - 1）
1 × S（与最小盘相同）
≤n × 单盘（读可并行）
1 × 单盘（需写入镜像）
最高数据安全性；个人 &#x2F; 企业重要数据、系统盘。磁盘利用率低。


RAID 2
3
1
(≈ n - 多个校验盘) × S
所有盘并行 (需同步)
所有盘并行 (高 ECC 开销)
理论级别，采用 ECC 汉明码。实际很少实现，了解概念为主。


RAID 3
3
1
(n - 1) × S
高 – 多盘并行顺序读
低 – 小随机需要全盘
中 – 顺序写并行，低 – 校验盘瓶颈


RAID 4
3
1
(n - 1) × S
中高 – 并行读不同盘
中等 – 写受限于校验盘
很少单独使用，曾用于特定存储系统。性能介于 RAID3 和 5 之间。


RAID 5
3
1
(n - 1) × S
高 – 近似 (n-1) 倍读
中等 – 顺序写接近 (n-1) 倍，随机写有校验开销
常用折衷方案，兼顾性能、容量和安全；个人和小型企业存储常用。


RAID 6
4
2
(n - 2) × S
高 – 近似 (n-2) 倍读
中下 – 写入有双校验较大开销
双冗余保障，高可靠企业存储常用。适合大容量、多盘环境，读多写少负载。


RAID 0+1
4
1 组失效
½ n × S
高 – 近似 RAID0 性能 (无故障)
高 – 近似 RAID0 性能 (无故障)
先条带后镜像。性能佳但故障后风险高。实际不常用，通常用 RAID10 代替。


RAID 1+0
4
每组 1 盘
½ n × S
高 – 接近 RAID0，随机读表现优秀
高 – 略低于 RAID0，随机写大幅优于 RAID5&#x2F;6
综合性能和容错均优秀。适合数据库等高 I&#x2F;O 应用。可承受非同组多盘故障。


RAID 50
6
每组 1 盘
(n - 组数) × S
高 – 多组并行，顺序读写优异
中等 – 组内写有校验，组间并行提升
多用于 &gt;6 盘的大型阵列，提升性能和重建并行度。容忍各组各 1 盘故障。


RAID 60
8
每组 2 盘
(n - 2×组数) × S
高 – 多组并行，顺序性能佳
中等偏低 – 组内双校验影响写入
超大型阵列使用，提供双重冗余和高吞吐。容忍各组各 2 盘故障。


RAID-DP
3-?
2
(n - 2) × S
高 – 类似 RAID4&#x2F;5 水平
中 – NetApp 优化后接近 RAID10
NetApp 专有双校验 RAID，企业存储默认配置。性能几乎不减，安全性高。


RAID-Z1&#x2F;Z2&#x2F;Z3
3&#x2F;4&#x2F;5
1&#x2F;2&#x2F;3
(n - 1&#x2F;2&#x2F;3) × S
高 – 近似 RAID5&#x2F;6 读性能
中 – 动态条带改善小写性能
ZFS 文件系统专用，消除 RAID5 写漏洞。单盘到三盘冗余可选，适合 NAS。


表注：上表中 n 表示阵列中磁盘总数。读写性能以相对于单盘的理论值表示，实际效能会因实现算法和工作负载不同而有所差异。RAID 10&#x2F;50&#x2F;60 等组合级别的容量公式需根据具体分组方式计算，表中给出的公式假定各组大小均等且使用标准奇偶算法。JBOD 列出的性能和安全性类似于不带冗余的 RAID 0。
RAID 的实现与配置方法RAID 可通过软件或硬件两种方式实现。软件 RAID 由操作系统或软件层管理阵列；硬件 RAID 则由独立的 RAID 控制器（包含处理器 &#x2F; 缓存）来管理阵列，对系统透明。下面分别介绍常见的软件 RAID 配置和硬件 RAID 配置方法。
软件 RAID 配置Linux 软件 RAID（mdadm）在 Linux 系统中，可使用内核提供的多设备阵列驱动（md）和工具 mdadm 来创建 RAID。配置步骤概述如下：

准备磁盘 &#x2F; 分区：确保待加入 RAID 的硬盘已连接，并对它们进行分区（可直接使用整盘或创建等大的分区）。多个磁盘最好容量相近，否则 RAID 容量会以最小盘为准。

创建阵列：使用 mdadm 命令创建 RAID 设备。例如，要创建一个包含 3 块磁盘的 RAID5 阵列，可以执行：
mdadm --create /dev/md0 --level=5 --raid-devices=3 /dev/sda1 /dev/sdb1 /dev/sdc1
其中 /dev/md0 是虚拟阵列设备，--level=5 指定 RAID5，--raid-devices=3 指定成员数，后面列出组成阵列的分区或磁盘。创建命令会触发构建过程，内核开始计算奇偶校验并初始化阵列（可以通过 watch cat /proc/mdstat 查看进度）。

创建文件系统并挂载：阵列设备建好后，像对待普通磁盘一样格式化（例如 mkfs.ext4 /dev/md0），然后挂载到系统某个挂载点即可使用。

配置开机自动组装：编辑 /etc/mdadm/mdadm.conf 添加阵列信息，并更新 initramfs（某些发行版自动完成）。这样重启时阵列会自动装配。同时可在 /etc/fstab 中添加阵列的挂载信息确保自动挂载。

监控和管理：mdadm 也用于后续管理，如查看状态（mdadm -D /dev/md0）、添加 &#x2F; 移除磁盘、设置热备盘（--add 命令）等。当阵列发生磁盘故障，mdadm 会发送内核事件，可配置 mdadm --monitor 或系统日志及时获知。更换故障盘后，用 mdadm --add 将新盘加入阵列进行重建。


Linux 的软件 RAID 具有良好性能和灵活性，可以实现 0,1,4,5,6,10 等级别，甚至支持在运行中调整阵列（如扩容 RAID5）。但需要消耗主机 CPU 进行校验计算。在现代 CPU 性能充裕的情况下，一般中低负载场景软件 RAID 绰绰有余。
Windows 软件 RAID（存储空间 &#x2F; 动态磁盘）Windows 操作系统提供了软件 RAID 功能，主要有两种实现方式：

动态磁盘模式（Legacy）：在早期 Windows 和当前专业版 Windows 中，可通过“磁盘管理”将基本磁盘转换为动态磁盘，从而创建带区卷（Striped, RAID0）、镜像卷（Mirrored, RAID1）或 RAID-5 卷。例如，在磁盘管理中选择两个空闲磁盘区可以创建一个镜像卷，实现 RAID1；选择三块动态磁盘可创建 RAID-5 卷。动态磁盘的 RAID 功能比较基础，例如 RAID5 卷仅在 Windows 服务器 &#x2F; 专业版可用，且管理不如现代方案便利。目前微软不再增强动态磁盘功能。
**存储空间 (Storage Spaces)**：这是 Windows 8&#x2F;10 及 Windows Server 提供的存储池功能。用户可以将多块磁盘加入一个存储池，然后在池上创建虚拟磁盘，指定所需的容错模式：例如“双向镜像”（2 盘镜像，相当于 RAID1）、“三向镜像”（3 副本）、“奇偶”（单奇偶 RAID5 类似）或“双奇偶”（类似 RAID6，两盘冗余）。存储空间支持精简配置和自动重新平衡等功能，使用 PowerShell 可以更细粒度控制列数（条带宽度）等参数。对于一般用户，通过控制面板或服务器管理器的向导即可配置。例如创建一个包含 4 盘的存储池，再新建一个“双奇偶”的虚拟盘，即得到一个具有两盘容错的冗余卷（类似 RAID6 的功能）。

Windows 软件 RAID 的配置通常比较直观，但需要注意版本限制（如 Win10 家用版不支持创建镜像卷）。存储空间在 Windows Server 系统中功能更强大，可替代部分硬件 RAID 需求。
其他软件 RAID 方案：除上述，某些操作系统或应用也提供软件 RAID 能力。例如 FreeBSD 的 gmirror&#x2F;graid，Linux 的 LVM 也能做 striping 或镜像（结合 md 更灵活），以及 UNRAID 等软件定义存储方案等。原则上软件 RAID 都依赖主机资源运行，优势是成本低、灵活方便。
软件 RAID 配置要点：使用软件 RAID 需确保硬盘连接稳定（直连 SATA&#x2F;SAS 或 JBOD 背板），并留意不要让主板的假 RAID 功能和软件 RAID 冲突。尽量在安装操作系统之前规划好阵列或使用能够在线迁移的方案。定期监控阵列状态，尤其在消费级操作系统上软件 RAID 故障通知可能不明显，需主动检查。
硬件 RAID 配置硬件 RAID 由专用的 RAID 控制器卡或主板上的 RAID 芯片负责管理阵列。硬件 RAID 控制器有自己的处理器和缓存，能在阵列上实现复杂运算且对操作系统透明（OS 看到的只是一块“大硬盘”）。配置硬件 RAID 一般通过控制卡的 BIOS&#x2F;UEFI 设置界面或厂商提供的管理软件完成：

安装 RAID 控制器 &#x2F; 硬盘连接：对于主板自带 RAID（如常见的 Intel RST、AMD RAID），在 BIOS 中启用 RAID 模式并将硬盘接到主板对应接口上。对于独立 RAID 卡（如 Dell PERC、LSI MegaRAID 等），将卡插入 PCIe 插槽并连接硬盘 &#x2F; 背板。启动服务器时通常会显示 RAID 卡 BIOS 提示，如“Press Ctrl+R to enter RAID Configuration”。
进入 RAID 设置界面：根据提示按下特定组合键进入 RAID 控制器的固件配置菜单。不同厂商界面略有不同，但基本提供创建阵列、删除阵列、查看阵列等选项。
创建 RAID 阵列：选择“创建阵列”（Create Array &#x2F; Create Virtual Disk 等）。然后选择要包含在阵列中的物理硬盘驱动器。通常可以选择将所有剩余盘都加入，或只选部分盘。
选择 RAID 级别：在可用 RAID 级别列表中，选定所需的 RAID 级别（0, 1, 5, 6, 10, 50, 60 等，具体取决于控制器型号支持）。控制器会根据所选级别和硬盘数验证是否可行（例如选 RAID5 需要至少 3 盘）。
设置阵列参数：通常可以设置条带大小（Stripe Size，如 64KB、128KB 等，影响顺序 IO 性能）、读写缓存策略（如 Write-Back 开关，如果控制器有电池 &#x2F; 闪存保护则可安全启用 Write-Back 缓存来提升写性能）、初始化方式（快速或全面初始化奇偶校验）等。还有些支持配置热备盘（Global Hot Spare），可指定某盘为全局热备，在任一阵列盘故障时自动介入重建。
确认并创建：保存配置，控制器将开始初始化阵列。如果是 RAID1&#x2F;5&#x2F;6 等有冗余的级别，通常需要初始化校验（写入零校验或同步镜像）——许多卡允许后台初始化，不影响先行使用阵列。创建完成后，控制器会将该阵列呈现给系统，主机 BIOS 会检测到一个新的逻辑驱动器。
系统使用：重启后，操作系统应能识别出这个逻辑磁盘。之后就像普通硬盘一样分区、格式化、安装系统或存储数据。如果是引导阵列，通常需要在安装操作系统时加载相应的 RAID 驱动（尤其独立 RAID 卡在 Windows 安装时需要提供驱动）。
管理维护：硬件 RAID 通常提供 OS 下管理工具或 web 界面（例如 MegaRAID Storage Manager）用于监控阵列健康、设置告警、管理重建等。也可以通过控制器 BIOS 监视。硬件 RAID 卡自带蜂鸣器或 LED 指示灯，可以指示故障硬盘的位置。发生硬盘故障时，控制器会自动使用热备盘（如果配置了）进行重建，或者等待管理员更换坏盘后手动触发重建。硬件 RAID 重建速度通常较快，因为控制器有专用处理器执行校验计算。

硬件 RAID 的优缺点：

优点：对 CPU 零占用、性能强劲（特别是带缓存和专用 ASIC 的高端卡，可加速校验计算），并且阵列管理独立于操作系统（阵列信息保存在卡和磁盘上，迁移到相同型号控制器上通常能直接识别阵列）。另外，一些硬件 RAID 提供高级功能如在线扩容、阵列级别迁移、快照等。
缺点：成本较高，专业 RAID 卡价格不菲且可能需要电池 &#x2F; 备用电源模块保障缓存安全；不同厂商实现有所差异，阵列难以跨品牌迁移；另外 RAID 卡本身也是单点，如果卡硬件故障且无法找到兼容替代，可能会导致阵列暂时不可用。

主板集成 RAID（又称 Fake RAID）注意：主板 BIOS 提供的 RAID 功能实际上介于软硬件之间——它依赖 BIOS 和驱动协同工作，并不具备独立处理器，大部分计算仍由 CPU 完成。此类 RAID 的好处是无需额外成本，坏处是兼容性和灵活性略差（例如换主板可能无法直接读阵列，需要相同芯片组），在 Linux 下往往还不如直接用 mdadm。对于桌面用户，主板 RAID 可用性一般，但在高级环境中更倾向于纯软件 RAID 或购买独立 RAID 卡。
总之，配置硬件 RAID 时务必保持相关文档在手，了解阵列的正确管理方法，特别是更换盘和升级固件要谨慎。硬件 RAID 控制器通常在性能和可靠性上表现出色，适合关键的企业应用场景。
RAID 级别选择指南面对不同数量的硬盘、应用需求和容错要求，应选择合适的 RAID 级别来平衡性能、容量和可靠性。以下是一些推荐原则和典型情形下的 RAID 选择建议：
不同磁盘数量下的 RAID 选择
只有单盘或容量不匹配多盘：可以选择不使用 RAID 或 JBOD。单盘无冗余，重要数据需做好备份。多块不同大小盘可以独立使用（JBOD 独立模式）或拼接成一个卷。如果追求最大容量且容错不重要，可用 JBOD 拼接来合并空间；但任一盘坏会影响整体卷，谨慎用于关键数据。
2 块硬盘：可选 RAID 1 或 RAID 0。如果数据重要需要冗余，RAID 1 镜像是唯一选择，能在一盘损坏时保护数据，但容量只有一半。若更看重性能或空间且有备份，可选 RAID 0 实现两盘条带化（读写性能翻倍，容量相加）。例如，系统盘或重要资料盘建议 RAID1，游戏或非关键高速存储可用 RAID0。
3 块硬盘：可考虑 RAID 5（2 盘数据 + 1 盘校验）。RAID 5 在 3 盘时提供冗余（可坏 1 盘）且比镜像利用率高（可用容量 2 盘，总容量的 66%）。适用于容量需求较大又希望有一定容错的小型存储。如果性能要求不高且想要最大容量，也可以 3 盘 JBOD 或 RAID0（不过任何一盘坏都会丢失数据，风险很高，不推荐存放重要数据）。另外，3 盘也可做 RAID 1 的“三盘镜像”（每盘都有相同数据），但这种用法少见且利用率低，仅在需要多个实时备份拷贝时才用。
4 块硬盘：常见选择有 RAID 5、RAID 6、RAID 10：
RAID 5：提供 1 盘冗余，可用容量 3 盘，利用率 75%。适合以读为主、希望较大容量的场景。
RAID 6：提供 2 盘冗余，可用容量 2 盘，利用率 50%。双冗余使安全性更高，但有效容量较少，4 盘 RAID6 在容量效率上不如直接两对 RAID1 镜像（也是 50%）——但 RAID6 胜在可容忍同时两盘故障而 RAID10 在 4 盘情况下只允许一对坏两盘。一般而言，4 盘阵列若非常看重可靠性，可选 RAID6；否则 RAID5 或 RAID10 会在性能或容量上更有优势。
RAID 10：2 对镜像再条带，可坏每对各一盘（最多坏 2 盘，容错略次于 RAID6 需分布恰当），容量 2 盘（利用率 50%）。RAID10 在 4 盘时提供了 RAID6 级别的冗余能力（耐 2 盘故障，但要分别在不同对）以及优于 RAID5&#x2F;6 的随机写性能，非常适合如数据库等需要高 IOPS 和可靠性的应用。不过可用容量只有 RAID5 的 2&#x2F;3。
推荐：若应用写入较多、要求高性能，4 盘用 RAID10；若应用读多写少、需要容量，4 盘 RAID5 较优；如果非常担心双盘同时故障（比如长时间无人值守环境），可以接受 50% 容量换安全，则 RAID6 是稳妥的方案。


5～6 块硬盘：随着磁盘增多，可以考虑 RAID 5 vs RAID 6 vs RAID 10 权衡：
5 盘 RAID5 可用 4 盘容量（80%），RAID6 可用 3 盘容量 (60%)，RAID10 可用 2.5 盘容量 (50%)。若需求容量为先，RAID5 的空间效率最高，但只能抗 1 盘坏，5 盘阵列稍有风险；RAID6 略降容量但提供双盘保护，常被认为更安全的选择尤其在 6 盘时（6 盘 RAID6 可坏 2 盘，利用率 ~67%）。RAID10 在 5 盘无法实现（需偶数盘），在 6 盘时可用 3 盘容量 (50%)，性能出众且可抗故障灵活（可坏 3 盘，概率依故障分布）。
推荐：一般 5 盘阵列建议用 RAID5（折中成本），但一定要做好备份；6 盘阵列更倾向于 RAID6（兼顾容量和安全）或 RAID10（对性能要求极高时）。例如，一个 6 盘的 NAS 用于备份存储，RAID6 会比 RAID5 更安心；而 6 盘做数据库 OLTP 存储，或需要很多随机写操作，则 RAID10 可能更适合。


8 块硬盘左右：可考虑 RAID 6 或 RAID 10，也可以探索 RAID 50&#x2F;60：
8 盘单 RAID6 可用 6 盘容量 (75%)，抗 2 盘故障；8 盘 RAID10 可用 4 盘容量 (50%)，抗故障能力取决于分布（最好情况 4 盘坏）；8 盘 RAID50 可比如分成两组 4 盘 RAID5 条带，可用 6 盘容量 (75%)，任两组各 1 盘坏（甚至一组坏 1 盘另一组坏 1 盘，总 2 盘坏）仍安全，但若一组坏 2 盘就失败；8 盘 RAID60 可分两组 4 盘 RAID6 条带，可用 4 盘容量 (50%)，任意每组最多坏 2 盘安全，甚至可能承受 4 盘坏（两组各 2 盘）。
推荐：如果追求最大容量且接受一定风险，可用 RAID6（是大多数 NAS&#x2F;存储在 8 盘左右的默认推荐）；如果追求性能或更高冗余，可考虑 RAID10，但代价是损失一半空间；如果有明确需求提升重建速度或 I&#x2F;O 并行，8 盘可以尝试 2 组 4 盘 RAID50：容量 75%，性能较高，但注意单组双盘故障风险；RAID60 则除非特别保守，一般不在仅 8 盘时使用（因为容量效率太低，RAID10 反而更简单高效）。


超过 10 块硬盘以上：阵列规模较大时，更应关注数据安全和重建窗口风险。此时推荐：
RAID 6 或 RAID 60 优先于 RAID5，双盘冗余可以防范在重建长过程中第二盘故障的灾难。
RAID 60 vs RAID 6：如果有硬件支持，多组 RAID6（RAID60）能将比如 16-24 盘划分成更小组管理，减少每组盘数以加快重建和降低多盘同组损坏概率。但 RAID60 利用率比单 RAID6 低一些，需要更多冗余盘。选择上，如果有 20 盘，做一个 18+2 RAID6 利用率 90% 但万一第三盘坏全丢；而做两组 9+2 RAID60 利用率 80% 但每组抗 2 盘坏且第三盘坏也未必丢（需同组第三盘坏才出问题），可靠性更强。对于关键业务，宁愿牺牲一些容量采用 RAID60 获得更高数据保障。
RAID 50 vs RAID 6：对于一些对性能要求较高的大阵列（尤其顺序 IO），RAID50 提供的并行优势更明显。如果系统有良好备份或容灾措施，也可以选择 RAID50 以获得比 RAID6 更好的写入性能和稍高容量。但要做好监控，确保及时更换故障盘，避免单组双盘故障。
RAID 10 &#x2F; 100：当磁盘非常多且应用极端重视性能（例如超大数据库、高频交易系统），可以考虑镜像 + 条带的多层次，例如 16 盘可以做 8 对镜像再条带（RAID10），32 盘甚至可以做两层：先镜像成 16 对，再把 16 对分成两组条带，然后两组再条带（这有时称 RAID100）。虽然利用率低（50%），但随机 I&#x2F;O 性能和数据安全都非常高（可容忍大量跨不同对的磁盘故障）。缺点是成本和空间代价巨大。一般只有在磁盘容量相对廉价且性能要求压倒一切时才会选如此配置。



特殊考虑
ZFS 等文件系统自带 RAID 方案：如果使用 ZFS 等有自己 RAID 方案的系统，优先采用 RAID-Z 系列而非硬件 RAID，将冗余交给文件系统管理，这样可以利用 ZFS 完整性校验、快照等特性，减少不同层次干扰。使用 RAID-Z 需要保证有足够内存和合理的 vdev 规划，比如不要一个 vdev 放太多盘（宁多 vdev 分摊）。典型 TrueNAS 配置如 12 盘可做 2 个 6 盘 RAID-Z2 vdev 组成池，兼顾可靠性和性能。
**热备盘 (Hot Spare)**：无论哪种 RAID 配置，若有剩余磁盘接口和预算，建议配置热备盘。热备盘是在阵列空闲待命的盘，一旦某阵列成员盘失败，控制器会自动把热备投入使用并开始重建，这可缩短阵列在降级状态下的时间，降低数据再次失效风险。一般 RAID5&#x2F;6 阵列至少配置 1 个全局热备盘较为稳妥。

最终选择建议最后，选择 RAID 级别需要结合具体应用：

数据库、虚拟化：这类小随机读写繁重的应用，更倾向 RAID 10。RAID 10 提供了高性能和高可靠性，适合需要高 IOPS 和快速读写性能的场景。
文件共享、归档：这类顺序读多于写的应用，RAID 5&#x2F;6 提供更大容量且性能足够。RAID 5&#x2F;6 在顺序读取性能上表现良好，同时提供了适当的冗余保护。
家庭 NAS：磁盘不多时，RAID 1 简单可靠，适合存放个人重要数据。当磁盘数量较多时，RAID 5&#x2F;6&#x2F;ZFS 更合适，能够提供更大的存储空间和冗余保护。
企业关键数据：一般至少采用 RAID6 级别冗余，宁可多花磁盘成本，以确保数据的安全性和可靠性。

重要提醒：无论选择哪种 RAID 级别，RAID 都不是备份手段。重要数据请一定另外备份或异地容灾，即使最高级别的 RAID 也无法抵御人为误删或灾难性损坏。
]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Raid</tag>
      </tags>
  </entry>
  <entry>
    <title>Sunshine + Moonlight + IPv6 公网串流</title>
    <url>/2025/06/10/stream/</url>
    <content><![CDATA[Sunshine 是一款轻量、开源的游戏串流​服务端​​软件，Moonlight 则是其互补的客户端方案。相比其他串流方案，​​二者能显著降低延迟并提升画质表现​。传统 IPv4 网络​​常受限于 NAT 配置，导致串流部署复杂化​​，而 IPv6 的​​原生公网直连特性（无需 NAT 穿透）​彻底解决了这一难题。本文​​将演示​​如何在 IPv6 环境下​​免配置端口映射​，​​快速实现​​ Sunshine + Moonlight ​​串流系统的部署​​。
环境准备


项目
说明



主机系统
Windows &#x2F; Linux


Sunshine版本
2025.122.141614 或更新


客户端
Moonlight（iOS &#x2F; Android &#x2F; Windows &#x2F; macOS &#x2F; Linux）


网络环境
IPv6 公网


检查 IPv6Windows打开 PowerShell 或 cmd，输入：
ipconfig
如果输出中有类似以下内容，说明启用了 IPv6：
以太网适配器 以太网:   IPv6 地址. . . . . . . . . . . . : 240e:1234:abcd::1   本地链接 IPv6 地址. . . . . . . .: fe80::xxxx:xxxx:xxxx:xxxx%xx

Linux终端通过 ip addr 命令来查询 IPv6 地址
ip addr | grep inet6
如果看到输出如下，说明有 IPv6
inet6 240e:1234:abcd::1/64 scope global dynamicinet6 fe80::xxxx:xxxx:xxxx:xxxx/64 scope link

安装 SunshineSunshine 的 Github 仓库为 Sunshine下载网址 Sunshine Releases
Linux包管理器安装Debian &#x2F; UbtuntuDebian &#x2F; Ubuntu 发行版可以直接使用 APT 包管理器进行安装。
sudo apt updatesudo apt install sunshine
如果遇到依赖问题（如 miniupnpc），可先执行：
sudo apt --fix-broken install
也可以从 Github 下载 deb 包来安装，以 Ubuntu 为例：
# 下载 Ubuntu 24.04 版本wget https://github.com/LizardByte/Sunshine/releases/download/v2025.122.141614/sunshine-ubuntu-24.04-amd64.deb# 安装 Sunshinedpkg -i sunshine-ubuntu-24.04-amd64.deb  
Debian 版本的也是类似：
https://github.com/LizardByte/Sunshine/releases/download/v2025.122.141614/sunshine-debian-bookworm-amd64.debdpkg -i sunshine-debian-bookworm-amd64.deb
如果下载慢或者下载失败，可以使用 Github 镜像站 GHPROXY.NET 来下载，以 Ubuntu 为例：
wget https://ghproxy.net/https://github.com/LizardByte/Sunshine/releases/download/v2025.122.141614/sunshine-ubuntu-24.04-amd64.debdpkg -i sunshine-ubuntu-24.04-amd64.deb
FedoraFedora 上可以从 copr 仓库安装 sunshine
# 启用 copr 仓库sudo dnf copr enable lizardbyte/stable     # 稳定版sudo dnf copr enable lizardbyte/beta       # Beta 测试版# 安装 sunshinesudo dnf install Sunshine
其他ArchLinux 版本有官方提供的预编译包，具体安装方式请参考在 ArchLinux 安装CentOS、Rocky 等类 Redhat 发行版则没有官方原生RPM包，可以尝试下面的 FlatPack 方式来安装。
AppImage将 sunshine.AppImage 下载到你的主目录：
cd ~wget https://github.com/LizardByte/Sunshine/releases/download/v2025.122.141614/sunshine.AppImage# 国内可以使用镜像站来加速下载wget https://ghproxy.net/https://github.com/LizardByte/Sunshine/releases/download/v2025.122.141614/sunshine.AppImage
安装
./sunshine.AppImage --install
运行
./sunshine.AppImage --install &amp;&amp; ./sunshine.AppImage
卸载
./sunshine.AppImage --remove
由于依赖库问题，不建议在 Rocky 和 CentOS 上使用 AppImage 来安装 Sunshine。
FlatPack此方式需要您已安装 FlatPack，安装方式可以参考 FlatPack 安装。 
离线安装# 下载 FlatPack 包wget https://github.com/LizardByte/Sunshine/releases/download/v2025.122.141614/sunshine_x86_64.flatpak # 使用 Github 镜像站加速下载wget https://ghproxy.net/https://github.com/LizardByte/Sunshine/releases/download/v2025.122.141614/sunshine_x86_64.flatpak# 安装 flatpak install --system ./sunshine_x86_64.flatpakflatpak run --command=additional-install.sh dev.lizardbyte.app.Sunshine
从 FlatHub 在线安装 flatpak install --system flathub dev.lizardbyte.app.Sunshineflatpak run --command=additional-install.sh dev.lizardbyte.app.Sunshine
运行
flatpak run dev.lizardbyte.app.Sunshine
WindowsWindows 版本提供安装程序 Sunshine Installer 和 便携版 Sunshine Portable 两种版本，本文以安装程序为例。下载并安装 Sunshine 安装程序 （国内镜像地址：GHPROXY）开始安装选择安装路径选择组件（默认全部就可以）至此，安装完成。
配置 SunshineWindows 版本安装完成之后会自动启动，Linux版本则可以通过 sunshine 命令来启动，启动成功后会显示 Web 管理页面的 URL。
[2025-06-11 15:52:34.628]: Info:[2025-06-11 15:52:34.628]: Info: // Ignore any errors mentioned above, they are not relevant. //[2025-06-11 15:52:34.628]: Info:[2025-06-11 15:52:34.628]: Info: Found H.264 encoder: libx264 [software][2025-06-11 15:52:34.631]: Info: Configuration UI available at [https://localhost:47990]
Sunshine 启动之后，可以访问 https://localhost:47990 来进入 Web 管理页面，首次登陆会要求输入管理账号和密码后续会通过 Basic 方式登录认证登陆成功后会看到下图所示的管理页面。接下来我们点击 Configuration -&gt; General -&gt; Locale 里选择简体中文，将语言改成中文，点击 Save -&gt; Apply 保存并应用，之后刷新网页，就能发现我们页面变成中文了。点击 Apply（应用）后刷新网页，可能会无法访问，此时在终端重启 Sunshine 主程序即可。
Sunshine 启用 IPv6在 配置 -&gt; NetWork -&gt; IP 地址族 选择 IPv4 + IPv6 ，保存 -&gt; 应用至此，Sunshine 配置结束了。
开放 Sunshine 服务端口Sunshine 默认使用的端口


服务
协议
端口范围
说明



Sunshine UI
TCP
47990
Sunshine Web 管理界面


Sunshine streaming
TCP
47984, 47989
控制和流媒体通道


Sunshine streaming
UDP
47998–48010
音视频流通道


Sunshine Discovery
UDP
47989, 1900
Moonlight 发现 Sunshine








一般情况下，Windows 是默认放行 Sunshine 相关端口的，而 Linux 则需要在 Firewalld 开放 Sunshine 使用到的端口，使其能被局域网中的其他设备连接，具体命令如下：





# 放行 TCP 端口sudo firewall-cmd --permanent --add-port=47984/tcp --add-port=47989/tcp --add-port=47990/tcp --add-port=48010/tcp# 放行 UDP 端口sudo firewall-cmd --permanent --add-port=47998-48000/udp# 应用策略sudo firewall-cmd --reload
现在，应该可以从同局域网中的其他设备的 Moonlight 上看到这台 Sunshine 主机了。
路由器开放端口我在这儿以 OpenWRT 为例，进入 网络 - 防火墙 - 通信规则，点击添加，名称按自己需求来写，协议根据 Sunshine 端口对应协议来写，源区域选择任意区域（转发），源地址留空，源端口选择任意，目标区域选择Lan，目的地址留空（因为IPv6地址不固定），目的端口填写对应的 Sunshine 服务端口，操作选择接受。切换到高级设置，地址族限制选择仅IPv6点击保存，保存并应用，至此，我们开放了一个 Sunshine 服务端口，相同方式放行所有 Sunshine 服务端口。至此端口开放端口完毕，可以直接使用 IPv6 来连接 Sunshine，教程到此结束了··？每次都输入IPv6地址将会非常麻烦，因此，我们还需要使用域名来解析我们 Sunshine 服务器的 IPv6 地址，这样就不需要记住那么长的 IPv6 地址了。
DDNS 配置前面我们已经在路由器上开放了 Sunshine  所需的端口，并可以直接使用 IPv6 地址从其他设备访问 Sunshine。但你可能已经意识到一个问题：

每次都输入一串复杂又难记的 IPv6 地址，不仅麻烦，而且容易输错。更重要的是，部分网络环境下，IPv6 地址是会变化的！  

这就引出了一个非常关键的优化点 —— 使用 DDNS（动态域名解析）服务。
为什么需要 DDNS？IPv6 虽然在局域网中通常不会频繁变化，但在部分网络环境下（如更换网络、设备重启、运营商分配机制等），IPv6 地址仍然有可能发生变更。此外，即使地址不变，它也不像 IPv4 那样简洁易记，常常是这样一串：
2408:xxx:3c4:fe10:ba2e:3eff:fe14:9f3a````在 Moonlight 或其他客户端中反复输入这样冗长的地址，既不直观，也容易出错。而 DDNS（动态域名解析）可以完美解决这个问题。### **DDNS 工具**先来总结一下常见的 DDNS 方式：| 方式类型              | 方案                          | 是否免费   | IPv6 支持 | 配置难度 | 依赖条件            | 特点说明                                        ||----------------------|---------------------------|------------|------------|------------|-----------------------|-------------------------------------------------|| 第三方服务            | **[DuckDNS](https://www.duckdns.org/)**| ✅ 免费     | ✅ 支持     | ★☆☆       | 无需域名             | 简单易用，国外节点                             || 第三方服务            | **[Dynu / No-IP](https://www.dynu.com/)**| ✅ 免费（部分需续期） | ✅ 支持     | ★★☆       | 无需域名 + 客户端     | 客户端可用，适合初学者                         || 自建脚本              | **Cloudflare API 脚本**       | ✅ 免费     | ✅ 支持     | ★★★       | 需域名 + Token        | 灵活性高，需动手写脚本                         || 自建脚本              | **AliDNS / DNSPod 脚本**      | ✅ 免费     | ✅ 支持     | ★★★       | 注册云服务             | 适合国内网络，脚本略复杂                       || 路由器自带            | ASUS、小米、OpenWRT DDNS 插件 | 多为免费    | 部分支持    | ★★☆       | 路由器型号相关         | 集成度好，设置受限                             || 自建服务（🔥 推荐）   | **[DDNSgo](https://github.com/jeessy2/ddns-go)**     | ✅ 免费开源 | ✅ 支持     | ★☆☆       | 可选域名平台          | Web UI 配置，支持多平台                         |这些方案里面，我强烈推荐 DDNS-GO ，后续教程也以 DDNS-GO 为例。#### **安装 DDNS-GO****DDNS-GO** 是一个轻量级、开源的 DDNS 工具，支持自动将本机的公网 IPv4 和 IPv6 地址同步到多个主流 DNS 平台（如 Cloudflare、阿里云、DNSPod 等）。它最大优点是**支持 Web UI 配置**，无需写脚本，适合新手快速上手，同时也支持多平台部署（包括群晖、Docker、Linux 等），非常适合家庭宽带和个人服务器使用。我们这次以二进制文件形式安装。  **下载地址：**- Windows：[Github](https://github.com/jeessy2/ddns-go/releases/download/v6.11.0/ddns-go_6.11.0_windows_x86_64.zip)，如果 Github 访问困难，可以通过镜像地址 [GHPROXY](https://ghproxy.net/https://github.com/jeessy2/ddns-go/releases/download/v6.11.0/ddns-go_6.11.0_windows_x86_64.zip) 来下载。- Linux：[Github](https://github.com/jeessy2/ddns-go/releases/download/v6.11.0/ddns-go_6.11.0_linux_x86_64.tar.gz)，如果 Github 访问困难，可以通过镜像地址 [GHPROXY](https://ghproxy.net/https://github.com/jeessy2/ddns-go/releases/download/v6.11.0/ddns-go_6.11.0_linux_x86_64.tar.gz) 来下载。      注：以上均为`x86_64`架构的二进制文件，如需其他架构，请自行从 Github 下载。**安装 DDNS 服务：**```bash# Linux 中tar -zxvf ddns-go_6.11.0_linux_x86_64.tar.gz  # 解压sudo ./ddns-go -s install                     # 安装 DDNS 服务# Windows 中# 自行解压 Zip 压缩包# 以管理员模式打开终端或 CMD，并执行以下命令.\ddns-go.exe -s install

获取 DNS 服务方 API 密钥下面我使用在阿里云购买的域名来配置 DDNS（注：国内域名需先进行备案，备案过程请自行查询），需已经在阿里云购买域名。  
创建AccessKey：访问 AccessKey 并登录，登陆成功后点击创建 AccessKey就可以获取AccessKey ID 和 AccessKey Secret，请妥善保存，AccessKey Secret 只会显示一次，请确保已保存好之后再点击确定。如果是通过子账号创建的 AccessKey ，可以在 RAM 访问控制里配置 AccessKey 的权限。接下来我们继续配置 DDNS-GO。
DDNS-GO 配置浏览器访问 http://localhost:9876 来进入 DDNS-GO 的 Web 控制界面，初次进入需配置用户名和密码，配置好之后，会进入到下面的页面：DDNS 服务商选择 阿里云，填写刚获取到的 AccessKey ID 和 AccessKey Secret，TTL 选择自动即可。

由于我们此次是对 IPv6 做 DDNS 解析，所以不需要启用 IPv4 。
启用 IPv6 的 DDNS 配置，获取 IP 方式默认（通过网卡获取）即可。
Domains 处填写解析 IPv6 地址的域名。配置完之后点击保存，至此 DDNS 配置完成。可以通过右上角的日志查看域名解析情况：


Moonlight 配置Moonlight 是一款开源的远程游戏串流客户端，基于 NVIDIA 的 GameStream 技术开发，支持在局域网或互联网中将主机上的游戏画面低延迟地传输到其他设备（如手机、平板、电视、笔记本）上。它支持 4K、60FPS、高码率传输，搭配 Sunshine 使用可不依赖 NVIDIA 显卡，实现高质量的跨平台远程游戏体验。
下载地址Windows&#x2F;Linux&#x2F;Mac：MoonLight-QtIOS&#x2F;AppleTV：AppStoreAndroid：GoogleStore，Github
连接 Sunshine这里以 Windows 版本的Moonlight 为例，其他版本的 Moonlight 也基本相同。点击右上角 + 号，输入之前配置的域名，如果连接成功，Sunshine 服务器就会出现在计算机列表：由于此时还未完成配对，会出现锁的图标，点击锁，会提示跟服务器匹配：此时在Sunshine 服务器上访问 https://localhost:47990 进入 Sunshine 后台，点击 Pin码，输入 Moonlight 上显示的 Pin码，设备名称可以按照自己的喜好来填写，点击发送，完成配对。此时打开 Moonlight，发现锁的图标消失了，说明配对成功，点击对应的 Sunshine 服务器就可以开始串流。    
至此，Moonlight 已连接 Sunshine，完成整个串流配置。
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Sunshine</tag>
        <tag>Moonlight</tag>
        <tag>串流</tag>
      </tags>
  </entry>
</search>
